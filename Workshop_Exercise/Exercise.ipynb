{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combient Workshop Exercise, June 22rd 2016\n",
    "\n",
    "You are given with the file housing.csv containing data about houses sold in the Seattle area. The goal of this exercise is to use Spark to build a model for predicting the sale price of a house based on information such as size, age, area, etc...\n",
    "\n",
    "You can find help in the Spark documentation https://spark.apache.org/docs/1.6.0/\n",
    "\n",
    "\n",
    "This example is inspired from the Coursera course [Machine Learning Foundations](https://www.coursera.org/learn/ml-foundations/) by University of Washington, and ported to Spark by Combient. \n",
    "\n",
    "-----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()   #This line is not needed on some platforms. Comment it out if it causes an error. \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line tells the notebook that we want all the figures as images inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook #Creates interactive plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "\n",
    "##### Option 1\n",
    "Use Pandas to read the file from the local HDD, then push it to Spark. (This will work only for small datasets) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_dataframe = pd.read_csv(...)\n",
    "pd_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_dataframe = sqlContext.createDataFrame(pd_dataframe)\n",
    "spark_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 2\n",
    "Use SFrame to read the file from the local HDD, then push it to Spark. (This will work for any datasets that fits on the HDD of the local computer.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sframe as sf\n",
    "sf_sframe = sf.SFrame.read_csv(...)\n",
    "sf_sframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_dataframe = sf_sframe.to_spark_dataframe(sc,sqlContext,2) #2 = number of partitions for the dataframe\n",
    "spark_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 3\n",
    "\n",
    "Read the file from HDFS directly using Spark and [pyspark_csv](https://github.com/seahboonsiew/pyspark-csv). This approach will work for truly big files. **Do not forget to put the file to HDFS first!**\n",
    "\n",
    "Simply sure the file pyspark_csv.py is in the same directory as the notebook and run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py') #Take the pyspark code and inject it into the python interpreters running on Spark nodes \n",
    "plainTextRdd = sc.textFile(...) #Read the file in a distributed fashion.\n",
    "spark_dataframe = pycsv.csvToDataFrame(sqlContext, plainTextRdd, parseDate=False)\n",
    "spark_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 4 [Will not work on the VM supplied]\n",
    "Read the file from HDFS directly using Spark and spark-csv, similarly to what we did in Scala for the NASA prototype.\n",
    "\n",
    "This is still work in progress on the VM we are using today ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark_dataframe = sqlContext.read.format('com.databricks.spark.csv').option('header', 'true').option('inferschema', 'true').load('housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Let's start by plotting the relation between the size of the house (\"sqft_living\") and the price.\n",
    "\n",
    "Extract the prices and sqft_living from the Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices_sqftliving = spark_dataframe.select(...,...).sample(False,0.1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What have we just done here ???\n",
    "\n",
    "Now, make a scatter plot using,with Pandas ( http://pandas.pydata.org/pandas-docs/version/0.15.0/visualization.html#scatter-plot )  or Seaborn (https://web.stanford.edu/~mwaskom/software/seaborn/generated/seaborn.jointplot.html#seaborn.jointplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices_sqftliving.plot(kind='scatter', x=..., y=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.jointplot(x=...,y=...,data=prices_sqftliving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for, lot size, number of bathroom, construction year and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices_others = spark_dataframe...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: In this dataset, bathrooms with a bath tub are counted as 1, bathrooms with only a shower are counted as 0.5 and those with only a toilet and sink are counted as 0.25. It is thus normal to see non-integer numbers of bathrooms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train and validation\n",
    "\n",
    "We split the data into training and test set. From now on, **you cannot touch the test set until the very end of the exercise!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(spark_DF_train,spark_DF_test) = spark_dataframe.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple linear regression\n",
    "\n",
    "Below, you are asked to perform a linear regression to predict the price from the living area.\n",
    "We will provide guidlines for Spark.mlib (the \"old way\" in Spark, using RDD) and Spark.ml (the \"new way\", using dataframes)\n",
    "\n",
    "\n",
    "### Spark.mlib\n",
    "\n",
    "https://spark.apache.org/docs/1.6.0/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the variables you want to use as features for the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_DF_price_sqftliv = spark_DF_train.select(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models in Spark.mlib take as input an RDD of LabeledPoints. A LabeledPoint is in turn a tuples of type (float, Vector[float]). The next lines maps the Dataframe create above into a suitable RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_RDD_price_sqftliv = spark_DF_price_sqftliv.map( lambda x : LabeledPoint(float(x.price),[ float(x.sqft_living)] ))\n",
    "spark_RDD_price_sqftliv.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the model!\n",
    "intercept is a boolean parameter telling if the linear regression crosses the origin or not, i.e. if the True, then we learn a model \"y = a1 * x + a0\"  and if false, \"y = a1 * x\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegressionWithSGD.train(...,intercept=...,iterations=1000,step=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, jump to Model evaluation / Spark.mlib section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark.ml\n",
    "\n",
    "This is similar to what we did in the Santander Bank's problem.\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression\n",
    "\n",
    "\n",
    "\n",
    "Create a list of names of columns that you want to use as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FeaturesCol = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annoying feature: the label needs to be of type DoubleType() while 'price' is of type int. The next blocks fixes that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "spark_DF_train = spark_DF_train.withColumn(\"price_float\",spark_DF_train[\"price\"].cast(DoubleType()))\n",
    "\n",
    "#Let's do it on the test data too so that we can forget about it.\n",
    "spark_DF_test = spark_DF_test.withColumn(\"price_float\",spark_DF_test[\"price\"].cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler,StandardScaler,MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#Create an empty list\n",
    "PipelineStages = ...\n",
    "\n",
    "#Create a vectorAssembler that packs the selected features into a vector in a column called \"features\"\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=...,\n",
    "    outputCol=...)\n",
    "\n",
    "#Append the vectorAssembler to the list of stages\n",
    "PipelineStages.append(...)\n",
    "\n",
    "#Optionally, normalize the data (do it first without)\n",
    "#feature_scaler = StandardScaler(inputCol=..., outputCol=...,withStd=..., withMean=...)\n",
    "#feature_scaler_model = feature_scaler.fit(...)\n",
    "#PipelineStages.append(feature_scaler_model)\n",
    "\n",
    "#From the doc: \n",
    "#class pyspark.ml.regression.LinearRegression(self, featuresCol=\"features\", labelCol=\"label\", \n",
    "#predictionCol=\"prediction\", maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \n",
    "#standardization=True, solver=\"auto\", weightCol=None)[source]\n",
    "lr = LinearRegression(...)\n",
    "\n",
    "#Add it to the pipeline: \n",
    "PipelineStages...\n",
    "\n",
    "#Create a pipeline for the model\n",
    "modelPipeline = Pipeline(stages=PipelineStages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data and fit by calling the .fit() method of the pipeline on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pipe_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "\n",
    "## Spark.mlib \n",
    "\n",
    "Let's start by looking at the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.weights\n",
    "print model.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this number reasonable ? Calculate the mean price/sqft! Does it look reasonable or completely off ? \n",
    "(Inside a lambda function, you can call the value of one column of a dataframe as \"variable.column_name\").\n",
    "\n",
    "TIP: What is the type of the object returned by map ? There is a very simple way of calculating its mean vallue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_DF_price_sqftliv.map(lambda x: ... ) ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression of one variable is simply a line! Can you plot it on the figure to get a feeling of how good the model is ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=\"sqft_living\",y=\"price\",data=prices_sqftliving,size=12) #Already done above\n",
    "g.ax_joint.plot(...,...,'--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the house price and calculate the mean squared error (MSE), first on the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the RDD from (price,features) to (price,prediction)\n",
    "spark_RDD_price_prediction = spark_RDD_price_sqftliv.map(lambda x: (x.label,model.predict(x.features)))\n",
    "spark_RDD_price_prediction.collect()[:10]   #Transform to a list and display        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the square error for each prediction (In python, square of x is x**2)\n",
    "spark_RDD_Error = spark_RDD_price_prediction.map(lambda x: ... ) \n",
    "\n",
    "from math import sqrt\n",
    "print \"sqrt(MSE) : %f\" %sqrt(spark_RDD_Error.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same on the test data. Remember that you have to prepare the data in the exact same way as you did with the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_DF_price_sqftliv_test = ...\n",
    "spark_RDD_price_sqftliv_test = ...\n",
    "spark_RDD_price_prediction_test = ...\n",
    "spark_RDD_Error_test= ...\n",
    "\n",
    "print \"sqrt(MSE) : %f\" %sqrt(spark_RDD_Error_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Annoying, isn't it ? =)\n",
    "\n",
    "## Spark.ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make predictions on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_with_predictions = Pipe_model.transform(spark_DF_train)\n",
    "train_with_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the MSE. Note that we are now using a Dataframe and not a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MSE = train_with_predictions.map(lambda x: ...) ....\n",
    "from math import sqrt\n",
    "print \"sqrt(MSE) : %f\" %sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A more general way of visualizing the result of the model is to plot the price vs relatve prediction error. \n",
    "\n",
    "Calculate the relative prediction error and add it to the dataframe as a new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_with_predictions = train_with_predictions.withColumn(\"error\",...)\n",
    "train_with_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Â£To be able to plot it, you have to retrieve the data from the cluster to the local machine as a Pandas dataframe.  Think about the datasize when you do this! (select only columns you need, sample the data if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_train_with_predictions = train_with_predictions.select(...).......toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(...,...,...,size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an idea model, the points should be distributed randomly around the y = 0 axis. It is not the case here...\n",
    "The model underestimates the price of cheap houses and overestimates the one of expensive ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_with_predictions = ...\n",
    "test_with_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't it easier than Spark.mlib ? =)\n",
    "\n",
    "\n",
    "Do the same error analysis for the test data and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate regression\n",
    "\n",
    "Copy-paste the code of one of the two approaches above and choose two (or more) new variables to add to the linear regression model. Check if this improves the predictive power of the model. \n",
    "\n",
    "If you use categorical variables as features, theses need to be indexed. An example can be founded in section 4 here: https://github.com/waichee/pyspark-ipython-notebook/blob/856b45eb17b2243bc08d3842659c6b44892256fa/spark-pyspark-mllib-101.ipynb\n",
    "Note that the distinction number/category isn't clear for every variables. For example, the number of bedrooms can be viewed as both a number or a category. On one hand, if taken as the only variables, more bedrooms is likely to always increase the price and using the number of bedrooms as a number is probably fine. On the other hand, for a given living area, more bedrooms also means smaller bedrooms, which might ultimetaly affect the price negatively, in which case a categorical view on the bedroom variable might be better. \n",
    "\n",
    "\n",
    "Finally, please note that when using variables with values in different orders of magnitude, normalization of the data before learning the model might be required (particularly if you use L1 or L2 relaxation).   http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.feature.StandardScalerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
